


import pandas as pd
import numpy as np
import boto3
import matplotlib.pyplot as plt
import seaborn as sns
import dtreeviz
import time
import pydotplus
from datetime import datetime as dt_obj
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import importlib
import dice_ml

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import cohen_kappa_score, make_scorer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn import preprocessing
from sklearn import svm
from sklearn.ensemble import GradientBoostingClassifier
from sklearn import metrics

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

pd.set_option('display.max_columns', 200)
pd.set_option('display.max_rows', 3000)





BUCKET_NAME = 'aae-water-dev-us-west-2'


s3 = boto3.resource('s3')
s3_client = boto3.client('s3')
water_bucket = s3.Bucket(BUCKET_NAME)


def get_data_from_s3(filename):
    for water_bucket_object in water_bucket.objects.all():
        response = s3_client.get_object(Bucket=BUCKET_NAME, Key=filename)
        status = response.get("ResponseMetadata", {}).get("HTTPStatusCode")
        print('Status equals {}, getting {}.'.format(status, filename))
        if status == 200:
            df = pd.read_csv(response.get("Body"))
            return(df)





df1 = get_data_from_s3(filename='DroughtWatchPrioritization_v4.csv')
df2 = get_data_from_s3(filename='DroughtWatchPrioritization_v9_input_data_with_TNC.csv')


df1[0:10]





df2 = df2.rename(columns={"Lat": "PWPhysicalAddressLat", "Long": "PWPhysicalAddressLong"})





df_2021_initial = df1.merge(df2, on=['ServiceConnections', 'Population', 'CDAG_rank', 'PWPhysicalAddressLat'])





df_2021_initial['HUC8'] = df_2021_initial['HUC8'].str.replace("-", "").astype(int)





df_2021_initial['Water Year'] = 2021





df_2021_initial = df_2021_initial.rename(columns={'EAR2020DroughtRiskScore_y':'EARDroughtRiskScore'}).reset_index(drop=True)


print("In 2021, ",df_2021_initial[df_2021_initial['DroughtExperienced_y'] == 1]['Population'].sum(),"people in California experienced some form of drought.")








url_huc8 = 'https://gis.data.cnra.ca.gov/datasets/02ff4971b8084ca593309036fb72289c_0.csv?outSR=%7B%22latestWkid%22%3A3857%2C%22wkid%22%3A102100%7D'


df_huc8 = pd.read_csv(url_huc8)


# Define the dry seasons
dry_seasons_start = [dt_obj.fromisoformat('2022-05-01'), dt_obj.fromisoformat('2021-05-01')]
dry_seasons_end = [dt_obj.fromisoformat('2022-09-30'), dt_obj.fromisoformat('2021-09-30')]

# Define the wet seasons
wet_seasons_start = [dt_obj.fromisoformat('2022-10-01'), dt_obj.fromisoformat('2021-10-01')]
wet_seasons_end = [dt_obj.fromisoformat('2023-04-01'), dt_obj.fromisoformat('2022-04-01')]





df_dsci = get_data_from_s3(filename='DSCI.csv')


if len(df_dsci[df_dsci['HUCId'] == 15030102].reset_index()) == 0:
    print("empty")


all_huc_ids = []
all_average_dscis_dry_season = []
all_average_dscis_wet_season = []
all_delta_dscis = []
years_average_dscis_dry_season = []
years_average_dscis_wet_season = []
years_delta_dscis = []
for i in range(len(df_huc8['HUC8'])):
    # Get an individual HUC8 region in California
    one_huc8_code = df_huc8['HUC8'][i]
    df_one_huc = df_dsci[df_dsci['HUCId'] == one_huc8_code].reset_index()
    if len(df_one_huc) == 0:
        print('Empty data for HUC8 region',one_huc8_code, df_huc8['Name'][i])
    else:
        print('Reading data for HUC8 region',one_huc8_code, df_huc8['Name'][i])
        # Convert the keyword MapDate into a datetime object
        df_one_huc['MapDate'] = pd.to_datetime(df_one_huc['MapDate'], format = '%Y%m%d').reset_index(drop=True)

        # Save the data to a list
        all_huc_ids.append(one_huc8_code)

        for j in range(len(dry_seasons_start)):
            # Identify one dry season
            mask_dry = (df_one_huc['MapDate'] > dry_seasons_start[j]) & (df_one_huc['MapDate'] <= dry_seasons_end[j])
            df_dry = df_one_huc[mask_dry]

            # Calculate the maximum, average, and standard deviation of the DSCI during the dry season
            average_dsci_dry_season = np.nanmean(1*df_dry.D0 + 2*df_dry.D1 + 3*df_dry.D2 + 4*df_dry.D3 + 5*df_dry.D4)
            standard_deviation_dry_season_dsci = np.nanstd(1*df_dry.D0 + 2*df_dry.D1 + 3*df_dry.D2 + 4*df_dry.D3 + 5*df_dry.D4)

            # Identify one wet season
            mask_wet = (df_one_huc['MapDate'] > wet_seasons_start[j]) & (df_one_huc['MapDate'] <= wet_seasons_end[j])
            df_wet = df_one_huc[mask_wet]

            # Calculate the average DSCI during the wet season
            average_dsci_wet_season = np.nanmean(1*df_wet.D0 + 2*df_wet.D1 + 3*df_wet.D2 + 4*df_wet.D3 + 5*df_wet.D4)

            # Calculate the change in average DSCI between dry and wet seasons
            delta_dsci = average_dsci_dry_season - average_dsci_wet_season

            # Concatenate each feature by years
            years_average_dscis_dry_season.append(average_dsci_dry_season)
            years_average_dscis_wet_season.append(average_dsci_wet_season)
            years_delta_dscis.append(delta_dsci)

        # Save the data to a list
        all_average_dscis_dry_season.append(years_average_dscis_dry_season)
        all_average_dscis_wet_season.append(years_average_dscis_wet_season)
        all_delta_dscis.append(years_delta_dscis)

        years_average_dscis_dry_season = []
        years_average_dscis_wet_season = []
        years_delta_dscis = []


df_drought_data_2021 = pd.DataFrame({
                   'HUC8': all_huc_ids,
                   'Water Year': [2021]*len(all_huc_ids),
                   'DSCI Dry Season Average': [item[0] for item in all_average_dscis_dry_season],        
                   'DSCI Wet Season Average': [item[0] for item in all_average_dscis_wet_season],
                   'DSCI Dry Wet Delta': [item[0] for item in all_delta_dscis],
})


df_drought_data_2022 = pd.DataFrame({
                   'HUC8': all_huc_ids,
                   'Water Year': [2022]*len(all_huc_ids),
                   'DSCI Dry Season Average': [item[1] for item in all_average_dscis_dry_season],        
                   'DSCI Wet Season Average': [item[1] for item in all_average_dscis_wet_season],
                   'DSCI Dry Wet Delta': [item[1] for item in all_delta_dscis],
})


df_drought_data_2021[df_drought_data_2021['HUC8'] == 18100204]


df_2021 = df_2021_initial.merge(df_drought_data_2021, on=['HUC8'], how='left')


df_2021['DSCI Dry Season Average'] = df_2021['DSCI Dry Season Average'].fillna(0)
df_2021['DSCI Wet Season Average'] = df_2021['DSCI Wet Season Average'].fillna(0)
df_2021['DSCI Dry Wet Delta'] = df_2021['DSCI Dry Wet Delta'].fillna(0)





df_SAFER_2021 = get_data_from_s3(filename='SAFER_2021.csv')


df_SAFER_2021 = df_SAFER_2021[['PWSID','Total']].rename(columns={'PWSID': 'CWSID', 'Total':'Total Number of Water Sources'})


df_2021 = df_2021.merge(df_SAFER_2021, how='left', on='CWSID').drop_duplicates().reset_index(drop=True)





df_2021['Total Number of Water Sources'] = df_2021['Total Number of Water Sources'].fillna(1)








df_2021.columns = df_2021.columns.str.removesuffix("_x")
df_2021.columns = df_2021.columns.str.removesuffix("_y")


# Drop duplicate columns
df_2021 = df_2021.loc[:,~df_2021.columns.duplicated()].copy()





# Redefine CentralValleyCounties
region_dictionary = {'No':0, 'BUTTE':1, 'COLUSA':1, 'FRESNO':1, 'GLENN':1, 'KERN':1, 'KINGS':1,
       'MADERA':1, 'MERCED':1, 'PLACER':1, 'SACRAMENTO':1, 'SAN JOAQUIN':1,
       'SHASTA':1, 'SOLANO':1, 'STANISLAUS':1, 'SUTTER':1, 'TEHAMA':1, 'TULARE':1,
       'YOLO':1, 'YUBA':1}
df_2021['CentralValleyCounties'] = df_2021['CentralValleyCounties'].apply(lambda x: region_dictionary[x])


# Redefine FracturedRock
fracturedrock_dictionary = {'0-000':0, 'FR-999':1}
df_2021['FracturedRock'] = df_2021['FracturedRock'].apply(lambda x: fracturedrock_dictionary[x])


# Redefine CurtailedWatershed
watershed_dictionary = {'NotInCurtailedWatershed':0, 'PartialSacSJ':1, 'PartialSJ':1,
       'SanJoaquinRiver':1, 'SacramentoRiver':1, 'SacSJ':1, 'RussianRiver':1,
       'PartialRus':1, 'PartialSac':1, 'ScottRiver':1, 'ShastaRiver':1,
       'PartialShasta':1}
df_2021['CurtailedWatershed'] = df_2021['CurtailedWatershed'].apply(lambda x: watershed_dictionary[x])


# Create HUC_Categories

df_2021['HUC8_CentralValley'] = df_2021['HUC8_CentralValley'].replace(to_replace='180*', value=0, regex=True)
df_2021['HUC8_Coast'] = df_2021['HUC8_Coast'].replace(to_replace='180*', value=0, regex=True)
df_2021['HUC8_Other'] = df_2021['HUC8_Other'].replace(to_replace='180*', value=0, regex=True)
df_2021['HUC8_Other'] = df_2021['HUC8_Other'].replace(to_replace='160*', value=0, regex=True)

df_2021 = df_2021.replace(to_replace='HUC8_Coast', value=1)
df_2021 = df_2021.replace(to_replace='HUC8_CentralValley', value=2)
df_2021 = df_2021.replace(to_replace='HUC8_Other', value=3)

df_2021['HUC_Categories'] = np.nan

df_2021['HUC_Categories'] = df_2021[['HUC8_CentralValley', 'HUC8_Coast', 'HUC8_Other']].max(axis=1, numeric_only=True)

df_2021 = df_2021.drop(columns=['HUC8_CentralValley', 'HUC8_Coast', 'HUC8_Other'])





# Rewrite all values of DroughtExperienced with np.NaN
df_2021['DroughtExperienced'] = np.NaN


df_2021['DroughtExperienced'].unique()


conditions = [
    (df_2021['WaterOutageDroughtViolation'].eq(1)) |
    (df_2021['BottledHauledWaterOrZeroSource'].eq(1)) |
    (df_2021['Score_1702'].eq(1)) |
    (df_2021['CurtailmentExemptionPetition'].eq(1)) |
    (df_2021['Score_1704'].eq(1)) |
    (df_2021['FundedProject'].eq(1))    
]

choicelist = [1]


df_2021['DroughtExperienced'] = np.select(conditions, choicelist, default=0)


df_2021['DroughtExperienced'].unique()


df_2021.shape








df_2021[['CWSID', 'ServiceConnections', 'Population', 'PWPhysicalAddressLat', 'PWPhysicalAddressLong']][30:39]





df_2021[df_2021['Population'] == 0.0][['CWSID','Population']]





keywords_to_use_in_the_model = [
 'CWSID',                   # Unique Identifier
 'Water Year',              # Unique Identifier
 'HUC8',                    # Unique Identifier
 'DroughtExperienced',      # Labels
 'WaterOutageDroughtViolation',
 'BottledHauledWaterOrZeroSource',
 'CurtailmentExemptionPetition',
 'FundedProject',            
 'DSCI Dry Season Average', # Drought Information
 'DSCI Wet Season Average',
 'DSCI Dry Wet Delta', 
 'Intertie',               # Infrastructure Information
 'ServiceConnections',
 'Population',
 'Score_SCV',              # Risk Scores
 'Score_SCM',
 'Curtailment',
 'Score_CriticallyOverdraftedGroundwaterBasin',
 'PWPhysicalAddressLong',  # Location Information
 'PWPhysicalAddressLat',
 'HUC_Categories',
 'Score_1702', 
 'Total Number of Water Sources'
]


df_2021 = df_2021[keywords_to_use_in_the_model]


df_2021.columns


df_2021.shape








df_2022 = df_2021[['CWSID', 'HUC8']]


df_2022.shape


df_2022 = df_2022.merge(df_drought_data_2022, on='HUC8', how='left')


df_2022['Water Year'] = 2022








df_outage = get_data_from_s3(filename='Water_Outage_Report_SDWISViolations.csv')


# Retain only three fields and rename water system id to CWSID
df_outage = df_outage[['water system id', 'Begin Date', 'End Date']].rename(columns={'water system id': 'CWSID'})


df_2022 = df_2022.merge(df_outage, how='left').drop_duplicates().reset_index(drop=True)


df_2022['WaterOutageDroughtViolation'] = np.NaN


# Convert the keywords Begin Date and End Date into a datetime object
df_2022['Begin Date'] = pd.to_datetime(df_2022['Begin Date'], format = '%Y-%m-%d')
df_2022['End Date'] = pd.to_datetime(df_2022['End Date'], format = '%Y-%m-%d')


# Identify the 2022 water season
# Define the dry season 2021
start_2022_water_season = dt_obj.fromisoformat('2021-10-01')
end_2022_water_season = dt_obj.fromisoformat('2022-09-30')





mask_2022_water_season_1 = (df_2022['Begin Date'] > start_2022_water_season) & (df_2022['End Date'] <= end_2022_water_season)
df_water_season_1 = df_2022[mask_2022_water_season_1]


df_water_season_1





mask_2022_water_season_2 = (df_2022['Begin Date'] != pd.NaT) & (df_2022['End Date'] == pd.NaT)
df_water_season_2 = df_2022[mask_2022_water_season_2]





df_2022.loc[mask_2022_water_season_1, ['WaterOutageDroughtViolation']] = 1





df_2022.loc[~mask_2022_water_season_1, ['WaterOutageDroughtViolation']] = 0


df_2022 = df_2022.drop(columns=['Begin Date', 'End Date'])


df_2022['WaterOutageDroughtViolation'] = df_2022['WaterOutageDroughtViolation'].astype('int').fillna(0)


df_2022.shape





df_bottled = get_data_from_s3(filename='2022BottledHauledWater_Drought Experienced.csv')
df_excluded = get_data_from_s3(filename='2022BottledHauledWater_Excluded.csv')


df_bottled = df_bottled.rename(columns={
    'PWSID': 'CWSID', 
    'Bottled Water or Hauled Water Reliance\r\n':'BottledHauledWaterOrZeroSource'
}).drop(columns=[
    'Number of Water Sources\r\n', 'Absence of Interties\r\n',
    'Source Capacity Violations\r\n',
    'DWR – Drought & Water Shortage Risk Assessment Results',
    'Critically Overdrafted Groundwater Basin',
    'Drought Experienced (Bottled Hauled Water Reliance)'
])


bottled_water_dictionary = {'Y':1}
df_bottled['BottledHauledWaterOrZeroSource'] = df_bottled['BottledHauledWaterOrZeroSource'].apply(lambda x: bottled_water_dictionary[x])


df_2022 = df_2022.merge(df_bottled, how='left', on='CWSID').drop_duplicates().reset_index(drop=True).fillna(0)


df_excluded


out = df_2022.loc[df_2022['CWSID'].isin(list(df_excluded['CWSID']))].replace(1,0)


df_2022.loc[list(out.index)] = df_2022.loc[list(out.index)].replace(1,0)


df_2022['BottledHauledWaterOrZeroSource'] = df_2022['BottledHauledWaterOrZeroSource'].astype('int').fillna(0)


df_2022.shape





df_funding_DFA_SHE = get_data_from_s3(filename='Funding_DFA.csv')
df_funding_DWR = get_data_from_s3(filename='Funding_DWR.csv')


df_funding_DFA_SHE = df_funding_DFA_SHE.drop(columns=[
    'Recipient​',
    'Project Type​',
    ' Funding Amount​ ',
    'Location (Lat/Long)',
    'Date Approved​',
    'Status',
    'Funded By']).rename(columns={'PWSID': 'CWSID', 'Drought Impacted Source': 'Drought Impacted Source DFA'})





df_funding_DFA_SHE_dictionary = {'Yes':1}
df_funding_DFA_SHE['Drought Impacted Source DFA'] = df_funding_DFA_SHE['Drought Impacted Source DFA'].apply(lambda x: df_funding_DFA_SHE_dictionary[x])





df_funding_DWR[df_funding_DWR['PWS ID'] == 'CA1610009'][['PWS ID', 'Applicant', 'Project Type', 'County', 'Drought Impacted Source']]


df_funding_DWR = df_funding_DWR.drop(columns=[
    'No.', 
    'Applicant',
    'Project Type',
    'County',
    'Description of Issue and Proposed Solution/Scope',
    'Recommended Funding, $', 'Column8', 'Population Served', 'Connections',
    '61510', 'Column12', 'Column13', 'Column14', 'RAA', 'Emails', 'Column1',
    'PSCODE']).rename(columns={'PWS ID': 'CWSID', 'Drought Impacted Source': 'Drought Impacted Source DWR'}).drop_duplicates().reset_index(drop=True)


df_funding_DWR_dictionary = {'Not Applicable':0, 'Yes':1, 'No':0, np.NaN: 0, 'Surface Water':1 , 'Well':1 }
df_funding_DWR['Drought Impacted Source DWR'] = df_funding_DWR['Drought Impacted Source DWR'].apply(lambda x: df_funding_DWR_dictionary[x])


df_funding_DWR = df_funding_DWR.drop_duplicates().reset_index(drop=True)





df_funding_DWR[df_funding_DWR['CWSID'].duplicated(keep=False) == True]


df_duplicated_DWR = df_funding_DWR[df_funding_DWR['CWSID'].duplicated(keep=False) == True]
df_funding_DWR.loc[list(df_duplicated_DWR.index)] = df_funding_DWR.loc[list(df_duplicated_DWR.index)].replace(0,1)
df_funding_DWR = df_funding_DWR.drop_duplicates().reset_index(drop=True)


df_funding = pd.concat([df_funding_DFA_SHE, df_funding_DWR]).drop_duplicates().reset_index(drop=True)





df_funding['Drought Impacted Source DFA'] = df_funding['Drought Impacted Source DFA'].fillna(0)
df_funding['Drought Impacted Source DWR'] = df_funding['Drought Impacted Source DWR'].fillna(0)





df_funding['FundedProject'] = df_funding['Drought Impacted Source DFA'] + df_funding['Drought Impacted Source DWR']





df_funding = df_funding.drop(columns=['Drought Impacted Source DFA','Drought Impacted Source DWR']).drop_duplicates().reset_index(drop=True)


df_duplicated_funding = df_funding[df_funding['CWSID'].duplicated(keep=False) == True]
df_funding.loc[list(df_duplicated_funding.index)] = df_funding.loc[list(df_duplicated_funding.index)].replace(0,1)
df_funding = df_funding.drop_duplicates().reset_index(drop=True)


df_2022 = df_2022.merge(df_funding, how='left', on='CWSID').drop_duplicates().reset_index(drop=True)





df_2022['FundedProject'] = df_2022['FundedProject'].fillna(0).astype('int')


df_2022.shape





df_curtailment = get_data_from_s3(filename='Curtailed.csv')


df_curtailment = df_curtailment.drop(columns=[
    'REG_AGENCY', 'COUNTY', 'PWS_NAME', 'WS_FED_TYPE',
    'SERVICE_CONNECTIONS', 'POPULATION', 'FAC_ID', 'FAC_NAME',
    'WR_TYPE', 'Claimed Priority Date', 'August Curtailment Status',
    'September Curtailment Status', 'DWR October Curtailment Status',
    'Watershed', 'Unnamed: 17', 'Exemptions',
    'Petitions', 'Unnamed: 20'
]).rename(columns={'WS_ID': 'CWSID', 'Exemption or Petition':'CurtailmentExemptionPetition', 'APPL_ID':'Curtailment'})


df_curtailment.columns


df_curtailment['CurtailmentExemptionPetition'].unique()


df_curtailment[df_curtailment['Curtailment'].isna()]


df_2022 = df_2022.merge(df_curtailment, how='left', on='CWSID').drop_duplicates().reset_index(drop=True)


df_exemption_dictionary = {np.NaN:0, 'Exemption':1, 'Petition - Approved':1, 'Petition - Withdrawn':0, 'Petition - Pending':1}
df_2022['CurtailmentExemptionPetition'] = df_2022['CurtailmentExemptionPetition'].apply(lambda x: df_exemption_dictionary[x])


df_2022['CurtailmentExemptionPetition'].value_counts()


conditions = [(~df_2022['Curtailment'].isna()),
              (df_2022['Curtailment'].isna())]

choicelist = [1,0]


df_2022['Curtailment'] = np.select(conditions, choicelist, default=0)


df_2022['Curtailment'].value_counts()


df_2022.shape





df_2021EAR = get_data_from_s3(filename='EAR.csv')


df_2021EAR = df_2021EAR[['WSID', '1702 Score', '1704']].rename(columns={'WSID': 'CWSID', '1702 Score': 'Score_1702', '1704': 'Score_1704'}).replace(2,1)


# Redefine 1704
Score_1704_dictionary = {'No':0, 'Yes':1, np.NaN: 0}
df_2021EAR['Score_1704'] = df_2021EAR['Score_1704'].apply(lambda x: Score_1704_dictionary[x])


df_2022 = df_2022.merge(df_2021EAR, how='left', on='CWSID').drop_duplicates().reset_index(drop=True)
df_2022['Score_1702'] = df_2022['Score_1702'].fillna(0)
df_2022['Score_1704'] = df_2022['Score_1704'].fillna(0)


df_2022.shape





conditions = [
    (df_2022['WaterOutageDroughtViolation'].eq(1)) |
    (df_2022['BottledHauledWaterOrZeroSource'].eq(1)) |
    (df_2022['Score_1702'].eq(1)) |
    (df_2022['CurtailmentExemptionPetition'].eq(1)) |
    (df_2022['Score_1704'].eq(1)) |
    (df_2022['FundedProject'].eq(1))    
]

choicelist = [1]


df_2022['DroughtExperienced'] = np.select(conditions, choicelist, default=0)


df_2022.shape








df_SDWIS_sources = get_data_from_s3(filename='Interties.csv')


df_SDWIS_sources = df_SDWIS_sources.drop(columns=[
    'Groundwater', 'Surface Water']).rename(columns={'PWSID': 'CWSID', 'Interties':'Number_Interties'})


conditions = [(df_SDWIS_sources['Number_Interties'].eq(0)),
              (df_SDWIS_sources['Number_Interties'].ne(0))]

choicelist = [0,1]


df_SDWIS_sources['Intertie'] = np.select(conditions, choicelist, default=0)


df_SDWIS_sources = df_SDWIS_sources.drop(columns=['Number_Interties'])


df_2022 = df_2022.merge(df_SDWIS_sources, how='left', on='CWSID')


df_2022['Intertie'] = df_2022['Intertie'].fillna(0)


df_2022.shape





df_SAFER = get_data_from_s3(filename='Drinking_Water_Risk_Assessment.csv')


df_SAFER = df_SAFER[['WATER_SYSTEM_NUMBER',
                     'SERVICE_CONNECTIONS',
                     'CRITICALLY_OVERDRAFTED_GROUNDWATER_BASIN_RAW_SCORE']].rename(columns={'WATER_SYSTEM_NUMBER': 'CWSID', 
                                                                                            'SERVICE_CONNECTIONS':'ServiceConnections', 
                                                                                            'CRITICALLY_OVERDRAFTED_GROUNDWATER_BASIN_RAW_SCORE':'Score_CriticallyOverdraftedGroundwaterBasin'})


df_SAFER = df_SAFER.replace('Not Assessed', np.NaN)


df_SAFER['Score_CriticallyOverdraftedGroundwaterBasin'] = df_SAFER['Score_CriticallyOverdraftedGroundwaterBasin'].fillna(0).astype('int')


df_SAFER.dtypes


df_2022 = df_2022.merge(df_SAFER, how='left', on='CWSID').drop_duplicates().reset_index(drop=True)


df_2022['Score_CriticallyOverdraftedGroundwaterBasin'] = df_2022['Score_CriticallyOverdraftedGroundwaterBasin'].fillna(0).astype('int')
df_2022['ServiceConnections'] = df_2022['ServiceConnections'].fillna(0)


df_2022.shape





df_SAFER_2022 = get_data_from_s3(filename='SAFER_2022.csv')


df_SAFER_2022 = df_SAFER_2022[['PWSID','Total Number of Water Sources ']].rename(columns={'PWSID': 'CWSID', 'Total Number of Water Sources ':'Total Number of Water Sources'})


df_2022 = df_2022.merge(df_SAFER_2022, how='left', on='CWSID').drop_duplicates().reset_index(drop=True)





df_2022['Total Number of Water Sources'] = df_2022['Total Number of Water Sources'].fillna(1)





df_SCV = get_data_from_s3(filename='Source_Capacity_Violations.csv')


df_SCV = df_SCV[['PWSID']].rename(columns={'PWSID': 'CWSID'})


df_SCV['Score_SCV'] = 1


df_2022 = df_2022.merge(df_SCV, how='left', on='CWSID').drop_duplicates().reset_index(drop=True)


df_2022['Score_SCV'] = df_2022['Score_SCV'].fillna(0)


df_2022.shape





df_SCM = get_data_from_s3(filename='Service_Connection_Moratoriums.csv')


df_SCM = df_SCM[['PWSID']].rename(columns={'PWSID': 'CWSID'})


df_SCM['Score_SCM'] = 1


df_2022 = df_2022.merge(df_SCM, how='left', on='CWSID').drop_duplicates().reset_index(drop=True)


df_2022['Score_SCM'] = df_2022['Score_SCM'].fillna(0)


df_2022.shape





df_wholesalers = get_data_from_s3(filename='SAFER_CLEARINGHOUSE_Wholesalers.csv')


df_wholesalers = df_wholesalers[['PWSID', 'PRIMARY_SERVICE_AREA_TYPE']].rename(columns={'PWSID':'CWSID'}).drop_duplicates().reset_index(drop=True)


df_wholesalers['PRIMARY_SERVICE_AREA_TYPE'].unique()


conditions = [df_wholesalers['PRIMARY_SERVICE_AREA_TYPE'].eq('WHOLESALER (SELLS WATER)      '),
              df_wholesalers['PRIMARY_SERVICE_AREA_TYPE'].ne('WHOLESALER (SELLS WATER)      ')]

choicelist = [1,0]


df_wholesalers['Wholesalers'] = np.select(conditions, choicelist, default=0)


df_wholesalers = df_wholesalers.drop(columns='PRIMARY_SERVICE_AREA_TYPE')


df_2022 = df_2022.merge(df_wholesalers, how='left', on='CWSID').drop_duplicates().reset_index(drop=True)


df_2022['Wholesalers'] = df_2022['Wholesalers'].fillna(0)


df_2022.shape








common_keys = np.intersect1d(df_2021.columns, df_2022.columns)


print('There are', len(common_keys),'common keys:')
print(*common_keys, sep=', ')





df_2022_subset = df_2022[common_keys]
df_2021_subset = df_2021[common_keys]





uncommon_keys_in_2022 = np.setdiff1d(df_2022.columns, df_2021.columns)
uncommon_keys_in_2021 = np.setdiff1d(df_2021.columns, df_2022.columns)


print('There are', len(uncommon_keys_in_2021),'keys unique to the 2021 dataset:')
print(*uncommon_keys_in_2021, sep=', ')


print('There are', len(uncommon_keys_in_2022),'keys unique to the 2022 dataset:')
print(*uncommon_keys_in_2022, sep=', ')





df_combined = pd.concat([df_2021_subset, df_2022_subset]).reset_index(drop=True)


df_combined.shape


print(2866*2)





for i in range(len(uncommon_keys_in_2022)):
    print(uncommon_keys_in_2022[i])
    df_combined = df_combined.merge(df_2022[[uncommon_keys_in_2022[i], 'CWSID']], how='left', on='CWSID').drop_duplicates().reset_index(drop=True)





for i in range(len(uncommon_keys_in_2021)):
    print(uncommon_keys_in_2021[i])
    df_combined = df_combined.merge(df_2021[[uncommon_keys_in_2021[i], 'CWSID']], how='left', on='CWSID').drop_duplicates().reset_index(drop=True)








df_combined[df_combined['ServiceConnections'] == 0][['CWSID', 'Water Year', 'ServiceConnections']]





df_combined[df_combined['CWSID'] == 'CA0400036'][['CWSID', 'Water Year', 'ServiceConnections']]








features = ['DSCI Dry Season Average',
'DSCI Wet Season Average',
'DSCI Dry Wet Delta',
'Intertie',
'Wholesalers',
'ServiceConnections',
'Population',
'Score_CriticallyOverdraftedGroundwaterBasin',
'Score_SCV',
'Score_SCM',
'Curtailment',
'PWPhysicalAddressLat',
'PWPhysicalAddressLong',
'HUC_Categories',
'Total Number of Water Sources']


labels = ['BottledHauledWaterOrZeroSource',
 'CurtailmentExemptionPetition',
 'DroughtExperienced',
 'FundedProject',
 'WaterOutageDroughtViolation',
 'Score_1702']


sns.set_theme(style="whitegrid")
fig, ax = plt.subplots(4,4, figsize=(19,12))
sns.kdeplot(df_combined, x='Curtailment', hue='Water Year', fill=True, ax=ax[0,0], legend=True, common_norm=False, palette='dark')
sns.kdeplot(df_combined, x='DSCI Dry Season Average', hue='Water Year', fill=True, ax=ax[0,1], legend=False, common_norm=False, palette='dark')
sns.kdeplot(df_combined, x='DSCI Wet Season Average', hue='Water Year', fill=True, ax=ax[0,2], legend=False, common_norm=False, palette='dark')
sns.kdeplot(df_combined, x='DSCI Dry Wet Delta', hue='Water Year', fill=True, ax=ax[0,3], legend=False, common_norm=False, palette='dark')
sns.kdeplot(df_combined, x='Intertie', hue='Water Year', fill=True, ax=ax[1,0], legend=False, common_norm=False, palette='dark')
sns.kdeplot(df_combined, x='Score_CriticallyOverdraftedGroundwaterBasin', hue='Water Year', fill=True, ax=ax[1,1], legend=False, common_norm=False, palette='dark')
sns.kdeplot(df_combined, x='Score_SCM', hue='Water Year', fill=True, ax=ax[1,2], legend=False, common_norm=False, palette='dark')
sns.kdeplot(df_combined, x='Score_SCV', hue='Water Year', fill=True, ax=ax[1,3], legend=False, common_norm=False, palette='dark')
sns.kdeplot(df_combined, x='ServiceConnections', hue='Water Year', fill=True, ax=ax[2,0], legend=False, log_scale=True, common_norm=False, palette='dark')
sns.kdeplot(df_combined, x='Wholesalers', hue='Water Year', fill=True, ax=ax[2,1], legend=False, common_norm=False, palette='dark')
sns.kdeplot(df_combined, x='HUC_Categories', hue='Water Year', fill=True, ax=ax[2,2], legend=False, common_norm=False, palette='dark')
sns.kdeplot(df_combined, x='PWPhysicalAddressLat', hue='Water Year', fill=True, ax=ax[2,3], legend=False, common_norm=False, palette='dark')
sns.kdeplot(df_combined, x='PWPhysicalAddressLong', hue='Water Year', fill=True, ax=ax[3,0], legend=False, common_norm=False, palette='dark')
sns.kdeplot(df_combined, x='Population', hue='Water Year', fill=True, ax=ax[3,1], legend=False, palette='dark')
sns.kdeplot(df_combined, x='Total Number of Water Sources', hue='Water Year', fill=True, ax=ax[3,2], legend=False, palette='dark')
plt.subplots_adjust(hspace=0.3, wspace=0.3)
#fig.savefig('Features_per_WaterYear.png', dpi=300, transparent=True, bbox_inches='tight')





fig, ax = plt.subplots(1,2, figsize=(10,4))
sns.kdeplot(x=df_combined['Curtailment'], fill=True, legend=True, ax=ax[0])
sns.kdeplot(x=df_combined['CurtailmentExemptionPetition'], fill=True, legend=True, ax=ax[1])


print('Number of Curtailments in 2021: ',df_combined[df_combined['Water Year'] == 2021]['Curtailment'].value_counts()[1])
print('Number of Petitions in 2021: ',df_combined[df_combined['Water Year'] == 2021]['CurtailmentExemptionPetition'].value_counts()[1])
print('Number of Curtailments in 2022: ',df_combined[df_combined['Water Year'] == 2022]['Curtailment'].value_counts()[1])
print('Number of Petitions in 2022: ',df_combined[df_combined['Water Year'] == 2022]['CurtailmentExemptionPetition'].value_counts()[1])





df_combined[df_combined['CurtailmentExemptionPetition'] == 1][['BottledHauledWaterOrZeroSource', 'WaterOutageDroughtViolation', 'Score_1702','Curtailment']]





print('Year 2021')
print('Positive values of DroughtExperienced: ',df_combined[df_combined['Water Year'] == 2021]['DroughtExperienced'].value_counts()[1])
print('Positive values of WaterOutageDroughtViolation: ',df_combined[df_combined['Water Year'] == 2021][['WaterOutageDroughtViolation']].value_counts()[1])
print('Positive values| of BottledHauledWaterOrZeroSource: ',df_combined[df_combined['Water Year'] == 2021][['BottledHauledWaterOrZeroSource']].value_counts()[1])
print('Positive values of CurtailmentExemptionPetition: ',df_combined[df_combined['Water Year'] == 2021][['CurtailmentExemptionPetition']].value_counts()[1])
print('Positive values of FundedProject: ',df_combined[df_combined['Water Year'] == 2021][['FundedProject']].value_counts()[1])
print('Positive values of FundedProject: ',df_combined[df_combined['Water Year'] == 2021][['Score_1702']].value_counts()[1])


print('Year 2022')
print('Positive values of DroughtExperienced: ',df_combined[df_combined['Water Year'] == 2022][['DroughtExperienced']].value_counts()[1])
print('Positive values of WaterOutageDroughtViolation: ',df_combined[df_combined['Water Year'] == 2022][['WaterOutageDroughtViolation']].value_counts()[1])
print('Positive values of BottledHauledWaterOrZeroSource: ',df_combined[df_combined['Water Year'] == 2022][['BottledHauledWaterOrZeroSource']].value_counts()[1])
print('Positive values of CurtailmentExemptionPetition: ',df_combined[df_combined['Water Year'] == 2022][['CurtailmentExemptionPetition']].value_counts()[1])
print('Positive values of FundedProject: ',df_combined[df_combined['Water Year'] == 2022][['FundedProject']].value_counts()[1])
print('Positive values of FundedProject: ',df_combined[df_combined['Water Year'] == 2022][['Score_1702']].value_counts()[1])
print('Positive values of FundedProject: ',df_combined[df_combined['Water Year'] == 2022][['Score_1704']].value_counts()[1])


total = [
    df_combined[df_combined['Water Year'] == 2021][['DroughtExperienced']].value_counts()[1],
    df_combined[df_combined['Water Year'] == 2021][['WaterOutageDroughtViolation']].value_counts()[1],
    df_combined[df_combined['Water Year'] == 2021][['BottledHauledWaterOrZeroSource']].value_counts()[1],
    df_combined[df_combined['Water Year'] == 2021][['CurtailmentExemptionPetition']].value_counts()[1],
    df_combined[df_combined['Water Year'] == 2021][['FundedProject']].value_counts()[1],
    df_combined[df_combined['Water Year'] == 2021][['Score_1702']].value_counts()[1],
    df_combined[df_combined['Water Year'] == 2021][['Score_1704']].value_counts()[1],    
    df_combined[df_combined['Water Year'] == 2022][['DroughtExperienced']].value_counts()[1],
    df_combined[df_combined['Water Year'] == 2022][['WaterOutageDroughtViolation']].value_counts()[1],
    df_combined[df_combined['Water Year'] == 2022][['BottledHauledWaterOrZeroSource']].value_counts()[1],
    df_combined[df_combined['Water Year'] == 2022][['CurtailmentExemptionPetition']].value_counts()[1],
    df_combined[df_combined['Water Year'] == 2022][['FundedProject']].value_counts()[1],
    df_combined[df_combined['Water Year'] == 2022][['Score_1702']].value_counts()[1],
    df_combined[df_combined['Water Year'] == 2022][['Score_1704']].value_counts()[1]]    


year = [2021, 2021, 2021, 2021, 2021, 2021, 2021,
        2022, 2022, 2022, 2022, 2022, 2022, 2022]


labels = ['DroughtExperienced', 'WaterOutageDroughtViolation', 'BottledHauledWaterOrZeroSource', 
          'CurtailmentExemptionPetition', 'FundedProject', 'Score_1702', 'Score_1704',
          'DroughtExperienced', 'WaterOutageDroughtViolation', 'BottledHauledWaterOrZeroSource', 
          'CurtailmentExemptionPetition', 'FundedProject', 'Score_1702', 'Score_1704']


df_examples = pd.DataFrame(
    {'Total': total,
     'Year': year,
     'Labels': labels})


fig, ax = plt.subplots(figsize=(14,4))
sns.set_theme(style='whitegrid')

sns.barplot(orient="h", data=df_examples, x='Total', y='Labels', hue='Year')
fig.savefig('outcome.png', dpi=300, transparent=True, bbox_inches='tight')





fig, ax = plt.subplots(figsize=(14, 14))
sns.set_theme(style="white")

# Calculate Spearman correlation coefficient
corr = df_combined.corr(method='spearman')
mask = np.triu(np.ones_like(corr, dtype=bool))

# Plot Spearman correlation coefficient as a diagonal heatmap
cmap = sns.color_palette('RdBu', as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, vmin=-1.0, center=0,
            square=True, linewidths=.75, cbar_kws={"shrink": .5, "label": 'Spearman Correlation Coefficient'})
#fig.savefig('spearman_with_labels_v4.png', dpi=300, transparent=True, bbox_inches='tight')





numeric_features = ['ServiceConnections', 'Population',
                    'PWPhysicalAddressLat', 'PWPhysicalAddressLong', 
                    'DSCI Dry Season Average', 'DSCI Wet Season Average',
                    'DSCI Dry Wet Delta', 'Total Number of Water Sources']

categorical_features = ['Score_SCV', 'Score_SCM',
                        'Score_CriticallyOverdraftedGroundwaterBasin',
                        'Intertie', 'Wholesalers',
                        'Curtailment', 'HUC_Categories']

all_features = numeric_features + categorical_features


# Use 2021 as the training data and 2022 as the testing data
training_index = df_combined[df_combined['Water Year'] == 2021].index
validation_index = df_combined[df_combined['Water Year'] == 2022].index


# Construct the examples
X = df_combined[all_features]
X_train = X.loc[training_index]
X_val = X.loc[validation_index]

# Construct the labels
y = df_combined['DroughtExperienced']
y_train = y.loc[training_index]
y_val = y.loc[validation_index]

print(f"The training set is {len(X_train)/len(X)*100}% of the data.")
print(f"The validation set is {len(X_val)/len(X)*100}% of the data.")





if (len(numeric_features) + len(categorical_features)) == len(X.columns):
    print('All variables are included.')





scaler = preprocessing.StandardScaler()

X_train_numeric = X_train[numeric_features]
X_val_numeric = X_val[numeric_features]





X_train_numeric_scaled = scaler.fit_transform(X_train_numeric) # Use fit_transform()
X_val_numeric_scaled = scaler.transform(X_val_numeric)         # Use transform()





X_train_categorical = np.array(X_train[categorical_features])
X_val_categorical = np.array(X_val[categorical_features])

X_train_scaled = np.concatenate((X_train_numeric_scaled, X_train_categorical), axis=1)
X_val_scaled = np.concatenate((X_val_numeric_scaled, X_val_categorical), axis=1)

X_train_scaled.shape, X_val_scaled.shape





# Print the number of examples per label in the training set
y_train.value_counts()


# Print the number of examples per label in the validation set
y_val.value_counts()


# Initialize the model
clf = svm.SVC(gamma='auto', kernel='rbf', C=5.0, class_weight={0:1, 1: 10}, probability=True)

# Fit the model
model = clf.fit(X_train_scaled, y_train)


# Predict the outcome with the best estimator
y_pred = y_pred = clf.predict(X_val_scaled)
y_pred_probability = clf.predict_proba(X_val_scaled)





fig, ax = plt.subplots()
rfc_disp = metrics.RocCurveDisplay.from_estimator(clf, X_val_scaled, y_val, ax=ax, response_method='predict_proba')
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
plt.show()
#fig.savefig('ROC_AUC.png', dpi=300, transparent=True, bbox_inches='tight')


fig, ax = plt.subplots()
rfc_disp = metrics.DetCurveDisplay.from_estimator(clf, X_val_scaled, y_val, ax=ax, response_method='predict_proba')
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('False Negative Rate')
ax.set_title('Detection Error Tradeoff (DET) Curve')
plt.show()
#fig.savefig('DET_2022.png', dpi=300, transparent=True, bbox_inches='tight')





y_val_np = np.array(y_val)


count_false_positive = 0
count_false_negative = 0
for i in range(len(y_val_np)):
    if ((y_val_np[i] == 0) and (y_pred_probability[i][0] <= 0.5)):
        count_false_positive += 1
        print('CWS {}: False Positive. True Value = {}, Probability of 0 = {:2.2%}, Probability of 1 = {:2.2%}.'.format(df_combined.loc[training_index][['CWSID']].iloc[i][0], y_val_np[i], (y_pred_probability[i][0]), y_pred_probability[i][1]))
    if ((y_val_np[i] == 1) and (y_pred_probability[i][1] <= 0.5)):
        count_false_negative += 1
        print('CWS {}: False Negative. True Value = {}, Probability of 0 = {:2.2%}, Probability of 1 = {:2.2%}.'.format(df_combined.loc[training_index][['CWSID']].iloc[i][0], y_val_np[i], y_pred_probability[i][0], y_pred_probability[i][1]))
print('There were {} false positives.'.format(count_false_positive))
print('There were {} false negatives.'.format(count_false_negative))





fig, ax = plt.subplots(figsize=(11,11))
sns.set_theme(style='whitegrid')

# Select method and fit
selector=SelectKBest(f_classif, k='all')
selector.fit(X, y)
scores_fisher = selector.scores_
scores_fisher[np.isnan(scores_fisher)] = 0.0

# Select, sort, and normalize features
features = X.columns.to_list()
order = np.argsort(scores_fisher)
ordered_features_fisher = [features[i] for i in order]
ordered_normalized_scores_fisher = sorted(scores_fisher/np.nanmax(scores_fisher))
ordered_scores_fisher = sorted(scores_fisher)

ax.barh(ordered_features_fisher, ordered_normalized_scores_fisher)
ax.set(xlabel = 'Normalized Fisher Score')
#fig.savefig('features.png', dpi=300, transparent=True, bbox_inches='tight')





# Construct the dataframe of examples with the label for DiCE
all_features_plus_label = all_features + ['DroughtExperienced']
X = df_combined[all_features_plus_label]
X_train_dice = X.loc[training_index]

# Create the DiCE data
d = dice_ml.Data(dataframe=X_train_dice, continuous_features=numeric_features, outcome_name='DroughtExperienced')


d.categorical_feature_names


d.continuous_feature_names


# Initialize the model
clf = svm.SVC(gamma='auto', kernel='rbf', C=6, class_weight={0:1, 1: 10}, probability=True)

# Fit the model
model = clf.fit(X_train, y_train)

# Create the DiCE model
m = dice_ml.Model(model=model, backend="sklearn")


m.model_type


m.model


m.backend


# Create the DiCE explainer
dice_explainer = dice_ml.Dice(d, m, method="kdtree")


# Generate counterfactuals
dice_counterfactuals = dice_explainer.generate_counterfactuals(X_val[0:1], 
                                                               total_CFs=2, desired_class="opposite")


dice_counterfactuals.visualize_as_dataframe(show_only_changes=False)


e1.visualize_as_list(show_only_changes=False)


query_instance = df_dice_val[0:1]
imp = exp.local_feature_importance(query_instance, total_CFs=10)
print(imp.local_importance)





def test_sample_weights(C, max_range, step_size):
    
    # Convert validation labels into a numpy array
    y_val_np = np.array(y_val)
    
    # Create an empty dataframe
    out = [np.NaN]*int(max_range/step_size)
    df_weight = pd.DataFrame({'C':out, 'weight':out,
                          'total_false_positives':out, 'total_false_negatives':out,
                          'total_misclassifications':out})
    
    # Define some counters
    count_misclassifications = 0
    count_false_positive = 0
    count_false_negative = 0
    count_true_positive = 0
    count_true_negative = 0
    total_counts = []
    total_false_positives = []
    total_false_negatives = []
    total_true_positives = []
    total_true_negatives = []
    weight = []

    print ("C:", C)
    
    for i in range(0, max_range, step_size):
            
            # Define a counter
            df_weight['C'].iloc[i]=C
            positive_class_weight = i+1
            df_weight['weight'].iloc[i] = positive_class_weight
            print("--> Sample weight:", positive_class_weight)

            # Initialize the model
            clf = svm.SVC(gamma='auto', kernel='rbf', C=C, class_weight={0:1, 1: positive_class_weight}, probability=True)

            # Fit the model
            clf.fit(X_train_scaled, y_train)
            y_pred = clf.predict(X_val_scaled)

            for j in range(len(y_val_np)):
                if y_val_np[j] != y_pred[j]:
                    count_misclassifications += 1
                if ((y_val_np[j] == 1.0) and (y_pred[j] == 0.0)):
                    count_false_negative += 1      
                if ((y_val_np[j] == 0.0) and (y_pred[j] == 1.0)):
                    count_false_positive += 1    
                if ((y_val_np[j] == 1.0) and (y_pred[j] == 1.0)):
                    count_true_positive += 1      
                if ((y_val_np[j] == 0.0) and (y_pred[j] == 0.0)):
                    count_true_negative += 1    

            df_weight['total_misclassifications'].iloc[i] = count_misclassifications
            df_weight['total_false_negatives'].iloc[i] = count_false_negative
            df_weight['total_false_positives'].iloc[i] = count_false_positive

            count_misclassifications = 0
            count_false_positive = 0
            count_false_negative = 0
            count_true_positive = 0
            count_true_negative = 0
            
    return df_weight


df_weights = []
for i in (0.5, 1.0, 2.0, 4.0, 8.0):
    df_weight = test_sample_weights(C = i, max_range=20, step_size=1)
    df_weights.append(df_weight)


for i in range(len(df_weights)):
    fig, ax = plt.subplots(figsize=(12,6))
    sns.set_theme(style='whitegrid')
    ax.errorbar(df_weights[i]['weight'], df_weights[i]['total_false_positives'], marker=".", markersize=10, label='False Positives')
    ax.errorbar(df_weights[i]['weight'], df_weights[i]['total_false_negatives'], marker=".", markersize=10, label='False Negatives')
    ax.set_ylabel('Misclassification')
    ax.set_xlabel('Positive Class Weight')
    C = (df_weights[i]['C'][0])
    ax.set_title('Misclassifications vs. Class Weight \n Regularization Parameter, C = '+str(C))
    ax.legend()
    ax.set_ylim(0,1100)
    ax.set_xlim(0,17)
    fig.savefig('weightsversusmisclassifications_C_'+str(C)+'_svm.png', dpi=300, transparent=True, bbox_inches='tight')


fig, ax = plt.subplots(figsize=(12,6))
sns.set_theme(style='whitegrid')
ax.errorbar(df_weights[0]['weight'].astype('int'), 
            df_weights[0]['total_false_negatives']/df_weights[0]['total_false_positives'],
            marker=".", markersize=10, label='C ='+str(df_weights[0]['C'][0]))
ax.errorbar(df_weights[1]['weight'].astype('int'), 
            df_weights[1]['total_false_negatives']/df_weights[1]['total_false_positives'],
            marker=".", markersize=10, label='C ='+str(df_weights[1]['C'][0]))
ax.errorbar(df_weights[2]['weight'].astype('int'), 
            df_weights[2]['total_false_negatives']/df_weights[2]['total_false_positives'],
            marker=".", markersize=10, label='C ='+str(df_weights[2]['C'][0]))
ax.errorbar(df_weights[3]['weight'].astype('int'), 
            df_weights[3]['total_false_negatives']/df_weights[3]['total_false_positives'],
            marker=".", markersize=10, label='C ='+str(df_weights[3]['C'][0]))
ax.errorbar(df_weights[4]['weight'].astype('int'), 
            df_weights[4]['total_false_negatives']/df_weights[4]['total_false_positives'],
            marker=".", markersize=10, label='C ='+str(df_weights[4]['C'][0]))
ax.set_ylabel('Misclassification Ratio \n (Total False Negatives : Total False Positives)')
ax.set_xlabel('Positive Class Weight')
ax.set_title('Misclassification Ratio vs. Class Weight')
ax.set_ylim(0,5)
ax.set_xlim(0,17)
ax.legend()
fig.savefig('weightsversusmisclassificationsratio_estimators_svm.png', dpi=300, transparent=True, bbox_inches='tight')





print("Number of positive examples in DroughtExperienced:",len(df_combined[df_combined['DroughtExperienced'] == 1]))
print("Number of positive examples in WaterOutageDroughtViolation:",len(df_combined[df_combined['WaterOutageDroughtViolation'] == 1]))
print("Number of positive examples in BottledHauledWaterOrZeroSource:",len(df_combined[df_combined['BottledHauledWaterOrZeroSource'] == 1]))
print("Number of positive examples in Score_1702:",len(df_combined[df_combined['Score_1702'] == 1]))
print("Number of positive examples in Score_1704:",len(df_combined[df_combined['Score_1704'] == 1]))
print("Number of positive examples in FundedProject:",len(df_combined[df_combined['FundedProject'] == 1]))


outcome_label = [
 'DroughtExperienced',
 'WaterOutageDroughtViolation',
 'BottledHauledWaterOrZeroSource',
 'Score_1702',
 'Score_1704',
 'FundedProject',
 'CurtailmentExemptionPetition']


%%capture

auc_values_per_label = []
auc_std_per_label = []
kappa_values_per_label = []
kappa_std_per_label = []

for k in range(len(outcome_label)):
    # Select the outcome label
    y = df_combined[outcome_label[k]]
    print(outcome_label[k])

    auc_values = []
    auc_std = []
    kappa_values = []
    kappa_std = []
    for i in range(len(ordered_features_fisher)):
        # Select the features
        X = df_combined[ordered_features_fisher[i:]]
        total_features = X.columns.to_list()

        auc_scores = []
        kappa_scores = []
        for j in range(3):    # This should eventually be 100
            # Use 2021 as the training data and 2022 as the testing data
            training_index = df_combined[df_combined['Water Year'] == 2021].index
            validation_index = df_combined[df_combined['Water Year'] == 2022].index

            X_train = X.loc[training_index]
            X_val = X.loc[validation_index]
            y_train = y.loc[training_index]
            y_val = y.loc[validation_index]
            
            # Identify the numeric and categorical features
            total_numeric_features = list(set(total_features).intersection(numeric_features))
            total_categorical_features = list(set(total_features).intersection(categorical_features))

            # Select and scale the numeric features
            if total_numeric_features:
                scaler = preprocessing.StandardScaler()
                X_train_numeric = X_train[total_numeric_features]
                X_val_numeric = X_val[total_numeric_features]
                X_train_numeric_scaled = scaler.fit_transform(X_train_numeric) # Use fit_transform()
                X_val_numeric_scaled = scaler.transform(X_val_numeric)         # Use transform()

            # Select the categorical features
            if total_categorical_features:
                X_train_categorical = np.array(X_train[total_categorical_features])
                X_val_categorical = np.array(X_val[total_categorical_features])

            # Create training and validation sets
            if total_numeric_features and total_categorical_features:
                X_train_scaled = np.concatenate((X_train_numeric_scaled, X_train_categorical), axis=1)
                X_val_scaled = np.concatenate((X_val_numeric_scaled, X_val_categorical), axis=1)
            if total_numeric_features and not total_categorical_features:
                X_train_scaled = X_train_numeric_scaled
                X_val_scaled = X_val_numeric_scaled
            if not total_numeric_features and total_categorical_features:
                X_train_scaled = X_train_categorical
                X_val_scaled = X_val_categorical

            # Train the model
            clf.fit(X_train_scaled, y_train)
            y_pred = clf.predict(X_val_scaled)

            # Calculate the AUC
            rfc_disp = metrics.RocCurveDisplay.from_estimator(clf, X_val_scaled, y_val, response_method='predict_proba');
            auc_scores.append(rfc_disp.roc_auc);
            
            # Calculate Cohen's Kappa Score
            kappa_score = metrics.cohen_kappa_score(y_val, y_pred)
            kappa_scores.append(kappa_score)
        
        auc_std.append(np.std(auc_scores))
        auc_values.append(np.mean(auc_scores))
        kappa_std.append(np.std(kappa_scores))
        kappa_values.append(np.mean(kappa_scores))
        print(i,np.mean(auc_scores), np.std(auc_scores))
        print(i,np.mean(kappa_scores), np.std(kappa_scores))

    auc_std_per_label.append(auc_std)
    auc_values_per_label.append(auc_values)
    kappa_std_per_label.append(kappa_std)
    kappa_values_per_label.append(kappa_values)


df_auc = pd.DataFrame({'AUC Scores': auc_values_per_label, 
                       'AUC Standard Deviation': auc_std_per_label,
                       'Kappa Scores': kappa_values_per_label, 
                       'Kappa Standard Deviation': kappa_std_per_label, 
                       'Outcome Label': outcome_label})


fig, ax = plt.subplots(figsize=(12,6))
sns.set_theme(style='whitegrid')
ax.errorbar(np.arange(len(df_auc['AUC Scores'][0])),np.flip(df_auc['AUC Scores'][0]), yerr=np.flip(df_auc['AUC Standard Deviation'][0]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][0])
ax.errorbar(np.arange(len(df_auc['AUC Scores'][1])),np.flip(df_auc['AUC Scores'][1]), yerr=np.flip(df_auc['AUC Standard Deviation'][1]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][1])
ax.errorbar(np.arange(len(df_auc['AUC Scores'][2])),np.flip(df_auc['AUC Scores'][2]), yerr=np.flip(df_auc['AUC Standard Deviation'][2]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][2])
ax.errorbar(np.arange(len(df_auc['AUC Scores'][3])),np.flip(df_auc['AUC Scores'][3]), yerr=np.flip(df_auc['AUC Standard Deviation'][3]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][3])
ax.errorbar(np.arange(len(df_auc['AUC Scores'][4])),np.flip(df_auc['AUC Scores'][4]), yerr=np.flip(df_auc['AUC Standard Deviation'][4]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][4])
ax.errorbar(np.arange(len(df_auc['AUC Scores'][5])),np.flip(df_auc['AUC Scores'][5]), yerr=np.flip(df_auc['AUC Standard Deviation'][5]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][5])
ax.errorbar(np.arange(len(df_auc['AUC Scores'][6])),np.flip(df_auc['AUC Scores'][6]), yerr=np.flip(df_auc['AUC Standard Deviation'][6]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][6])
ax.set_xlabel('Features')
ax.set_ylabel('Performance (AUC)')
ax.set_title('Performance vs. Number of Features')
ax.set_ylim([0.3, 1.00])
ax.legend()
fig.savefig('performancevsfeaturesvslabel_auc_gb.png', dpi=300, transparent=True, bbox_inches='tight')


fig, ax = plt.subplots(figsize=(12,6))
sns.set_theme(style='whitegrid')
ax.errorbar(np.arange(len(df_auc['Kappa Scores'][0])),np.flip(df_auc['Kappa Scores'][0]), yerr=np.flip(df_auc['Kappa Standard Deviation'][0]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][0])
ax.errorbar(np.arange(len(df_auc['Kappa Scores'][1])),np.flip(df_auc['Kappa Scores'][1]), yerr=np.flip(df_auc['Kappa Standard Deviation'][1]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][1])
ax.errorbar(np.arange(len(df_auc['Kappa Scores'][2])),np.flip(df_auc['Kappa Scores'][2]), yerr=np.flip(df_auc['Kappa Standard Deviation'][2]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][2])
ax.errorbar(np.arange(len(df_auc['Kappa Scores'][3])),np.flip(df_auc['Kappa Scores'][3]), yerr=np.flip(df_auc['Kappa Standard Deviation'][3]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][3])
ax.errorbar(np.arange(len(df_auc['Kappa Scores'][4])),np.flip(df_auc['Kappa Scores'][4]), yerr=np.flip(df_auc['Kappa Standard Deviation'][4]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][4])
ax.errorbar(np.arange(len(df_auc['Kappa Scores'][5])),np.flip(df_auc['Kappa Scores'][5]), yerr=np.flip(df_auc['Kappa Standard Deviation'][5]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][5])
ax.errorbar(np.arange(len(df_auc['Kappa Scores'][6])),np.flip(df_auc['Kappa Scores'][6]), yerr=np.flip(df_auc['Kappa Standard Deviation'][6]), 
            marker=".", markersize=10, label=df_auc['Outcome Label'][6])
ax.set_xlabel('Features')
ax.set_ylabel('Performance (Cohen\'s Kappa Score)')
ax.set_title('Performance vs. Number of Features')
ax.set_ylim([0.0, 1.00])
ax.legend()
fig.savefig('performancevsfeaturesvslabel_Kappa_gb.png', dpi=300, transparent=True, bbox_inches='tight')



